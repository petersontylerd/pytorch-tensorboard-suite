{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mh1KdylETHB6"
   },
   "source": [
    "# Import tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 128
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 24235,
     "status": "ok",
     "timestamp": 1576178366862,
     "user": {
      "displayName": "Tyler Peterson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDnpfdQwULzqeCl69rWVBKY5JSpseOg_yT2QyrbGg=s64",
      "userId": "06972126212785161073"
     },
     "user_tz": 360
    },
    "id": "Q2h8SmrccBSa",
    "outputId": "df1f7c07-3214-4475-89d3-4ceacdcc95b9"
   },
   "outputs": [],
   "source": [
    "# built-in utilities\n",
    "import copy\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import pickle\n",
    "\n",
    "# data tools\n",
    "import numpy as np\n",
    "\n",
    "# pytorch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import datasets, models, transforms\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import Image, display\n",
    "\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8AT0s01yV3c"
   },
   "source": [
    "# Report\n",
    "\n",
    "Question 2 of homework 4 tasks us with using a variational auto-encoder  (VAE) to determine what augmentation was applied to a set of images containing handwritten 7’s from the MNIST dataset. We are given a training dataset containing 6,265 7’s and a test set containing 1,028 additional 7’s. We are not informed of the augmentation that was applied to the images.\n",
    "The VAE architecture is as follows:\n",
    "-\tEncoder\n",
    "    -\t2-D convolutional layer\n",
    "        -\tInput = 1, Output = 64, Kernel size = 4, Stride = 2, Padding = 1\n",
    "    -\t2-D convolutional layer\n",
    "        -\tInput = 64, Output = 128, Kernel size = 4, Stride = 2, Padding = 1\n",
    "    -\t2-D convolutional layer\n",
    "        -\tInput = 128, Output = 256, Kernel size = 3, Stride = 2, Padding = 1\n",
    "    -\t2-D convolutional layer\n",
    "        -\tInput = 256, Output = 1024, Kernel size = 4, Stride = 1, Padding = 0\n",
    "-\tDecoder\n",
    "    -\t2-D transposed convolutional layer\n",
    "        -\tInput = 1024, Output = 512, Kernel size = 4, Stride = 1, Padding =\n",
    "0    - 2-D transposed convolutional layer\n",
    "        - Input = 512, Output = 256, Kernel size = 3, Stride = 2, Padding = 1\n",
    "    - 2-D transposed convolutional layer\n",
    "        - Input = 256, Output = 128, Kernel size = 4, Stride = 2, Padding = 1\n",
    "    - 2-D transposed convolutional layer\n",
    "        - Input = 128, Output = 1, Kernel size = 4, Stride = 2, Padding = 1\n",
    "- Fully connected layers\n",
    "    - Linear layer 1\n",
    "        - Input = 1,024, Output = 512\n",
    "    - Linear layer 2 (mean)\n",
    "        - Input = 512, Output = 3\n",
    "    - Linear layer 2 (standard deviation)\n",
    "        - Input = 512, Output = 3\n",
    "    - Linear layer 3\n",
    "        - Input = 3, Output = 512\n",
    "    - Linear Layer 4\n",
    "        - Input = 3, Output = 1024\n",
    "\n",
    "The number of latent dimensions is 3. Over 25 epochs, the training loss decreases from 187.2966 after the first epoch to 101.7138 at the 25th epoch, and the test loss decreases from 122.6691 after the first epoch to 100.9801 at the 25th epoch. Most of the loss decrease occurs for the binary cross entropy side of the loss function, as opposed to the KLD side of the loss function.\n",
    "\n",
    "As for reconstructing the images to determine the effect that was added to the image, I am going to have to attempt this outside of the scope of the homework, as I have run out of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GnUT0lOhVM55"
   },
   "source": [
    "# Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uTlXfthvVNZq"
   },
   "source": [
    "## Question 1, part 1 - 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2240,
     "status": "ok",
     "timestamp": 1576178368494,
     "user": {
      "displayName": "Tyler Peterson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDnpfdQwULzqeCl69rWVBKY5JSpseOg_yT2QyrbGg=s64",
      "userId": "06972126212785161073"
     },
     "user_tz": 360
    },
    "id": "atHX-rnjwkXC",
    "outputId": "99d6bc48-da4e-4d0f-c763-451aacf3118f"
   },
   "outputs": [],
   "source": [
    "with open('hw4_tr7.pkl', 'rb') as f:\n",
    "    train_data_raw = pickle.load(f)\n",
    "    print(\"Train data shape: {}\".format(train_data_raw.shape))\n",
    "\n",
    "with open('hw4_te7.pkl', 'rb') as f:\n",
    "    test_data_raw = pickle.load(f)\n",
    "    print(\"Test data shape: {}\".format(test_data_raw.shape))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bCugrZiazHkh"
   },
   "outputs": [],
   "source": [
    "class FullDataset(Dataset):\n",
    "  def __init__(self, data):\n",
    "        self.data = torch.from_numpy(data).unsqueeze(1)\n",
    "        self.target = torch.ones(data.shape[0]) * 7\n",
    "        \n",
    "  def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        data = self.data[index]\n",
    "        target = self.target[index]\n",
    "        return data, target\n",
    "\n",
    "train_data = FullDataset(train_data_raw)\n",
    "test_data = FullDataset(test_data_raw)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1576178442186,
     "user": {
      "displayName": "Tyler Peterson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDnpfdQwULzqeCl69rWVBKY5JSpseOg_yT2QyrbGg=s64",
      "userId": "06972126212785161073"
     },
     "user_tz": 360
    },
    "id": "ZcTO-px8xXJh",
    "outputId": "e0730de8-0664-4141-d5e8-aef868b033ef"
   },
   "outputs": [],
   "source": [
    "## load data\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=128, shuffle=True)\n",
    "\n",
    "train_samples, _ = next(iter(train_loader))\n",
    "save_image(train_samples.data, \"test.png\", nrow=8)\n",
    "Image(\"test.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7sbdB67F3PTf"
   },
   "source": [
    "## Question 1, part 4 - 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h8MPrFn_AuE1"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=3):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # Input = Batch size x 1 x 28 x 28\n",
    "            \n",
    "            # Batch size x 64 x 14 x 14\n",
    "            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Batch size x 128 x 7 x 7\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Batch size x 256 x 4 x 4\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            \n",
    "            # Batch size x 1024 x 1 x 1\n",
    "            nn.Conv2d(in_channels=256, out_channels=1024, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(1024),            \n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            # Input =  Batch size x 1024 x 1 x 1\n",
    "            \n",
    "            # Batch size x 512 x 4 x4\n",
    "            nn.ConvTranspose2d(in_channels=1024, out_channels=512, kernel_size=4, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Batch size x 256 x 7 x 7\n",
    "            nn.ConvTranspose2d(in_channels=512, out_channels=256, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Batch size x 128 x 14 x 14\n",
    "            nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            \n",
    "            # Batch size x 1 x 28 x 28\n",
    "            nn.ConvTranspose2d(in_channels=128, out_channels=1, kernel_size=4, stride=2, padding=1),\n",
    "            # nn.Tanh()\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Linear(1024, 512)\n",
    "        \n",
    "        self.fc2m = nn.Linear(512, self.latent_dim)\n",
    "        self.fc2s = nn.Linear(512, self.latent_dim)\n",
    "        self.fc3 = nn.Linear(self.latent_dim, 512)\n",
    "\n",
    "        self.fc4 = nn.Linear(512, 1024)\n",
    "\n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = self.fc1(h.view(-1, 1024))\n",
    "        \n",
    "        mu, logvar = self.fc2m(h), self.fc2s(h)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        return z, mu, logvar\n",
    "    \n",
    "    def decode(self, z):\n",
    "        z = F.relu(self.fc3(z))\n",
    "        z = self.fc4(z)\n",
    "        \n",
    "        z = z.view(-1, 1024, 1, 1)\n",
    "        z = self.decoder(z)\n",
    "        return z\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = logvar.mul(0.5).exp_()\n",
    "        esp = torch.randn(*mu.size()).to(device)\n",
    "        z = esp.mul(std).add_(mu)\n",
    "        return z\n",
    "\n",
    "    def forward(self, x):\n",
    "        z, mu, logvar = self.encode(x)\n",
    "        z = self.decode(z)\n",
    "        return z, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 135678,
     "status": "ok",
     "timestamp": 1576178876880,
     "user": {
      "displayName": "Tyler Peterson",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDnpfdQwULzqeCl69rWVBKY5JSpseOg_yT2QyrbGg=s64",
      "userId": "06972126212785161073"
     },
     "user_tz": 360
    },
    "id": "3z2xi_2FTWmu",
    "outputId": "8c31af4b-00ca-4950-87e2-bdff4ba9a87b"
   },
   "outputs": [],
   "source": [
    "## load data\n",
    "batch_size = 32\n",
    "epochs = 25\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "model = VAE(latent_dim=4).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def loss_fn(recon_x, x, mu, logvar, fn=\"BCE\"):\n",
    "    if fn == \"BCE\":\n",
    "        BCE = F.binary_cross_entropy(recon_x, x, reduction=\"sum\")\n",
    "    elif fn == \"MSE\":\n",
    "        BCE = F.mse_loss(recon_x, x, reduction=\"sum\")\n",
    "    \n",
    "    KLD = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE + KLD, BCE, KLD\n",
    "\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        \n",
    "        recon_images, mu, logvar = model(images)\n",
    "        loss, bce, kld = loss_fn(recon_images, images, mu, logvar)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            print(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tTotal Loss: {:.5f}  \\tBCE Loss: {:.5f} \\tKLD Loss: {:.5f}\".format(\n",
    "                    epoch + 1,\n",
    "                    batch_idx * batch_size,\n",
    "                    len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.item() / batch_size,\n",
    "                    bce.item() / batch_size,\n",
    "                    kld.item() / batch_size\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    print('\\nEpoch: {} Average train loss: {:.4f}'.format(epoch + 1, train_loss / len(train_loader.dataset)))\n",
    "    torch.save(model.state_dict(), 'hw4_2.pt')\n",
    "\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss= 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, _) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            \n",
    "            recon_images, mu, logvar = model(images)\n",
    "            loss, bce, kld = loss_fn(recon_images, images, mu, logvar)  \n",
    "                        \n",
    "            test_loss += loss\n",
    "        \n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Epoch: {} Average test loss: {:.4f}\\n\\n'.format(epoch + 1, test_loss))\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train(epoch)\n",
    "    test(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0SutZ2fZ3vPP"
   },
   "source": [
    "## Question 1, part 7 - 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1IAu9Yoj3yiO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW4_Q2.ipynb",
   "provenance": [
    {
     "file_id": "1xVa7tj528B5hmaLc4bDACWJGfRlHiT-r",
     "timestamp": 1569641412553
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
