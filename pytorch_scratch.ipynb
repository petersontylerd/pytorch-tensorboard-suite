{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T02:29:05.520721Z",
     "start_time": "2020-06-05T02:28:55.624454Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from itertools import product\n",
    "from collections import namedtuple, OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "tensorboard_log_dir = os.path.join(os.environ[\"HOME\"],\"workspace\",\"tensorboard_logdir\")\n",
    "\n",
    "from sklearn.model_selection import KFold, train_test_split, GridSearchCV, StratifiedKFold, cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, explained_variance_score, mean_squared_log_error, mean_absolute_error, median_absolute_error, mean_squared_error, r2_score, confusion_matrix, roc_curve, accuracy_score, roc_auc_score, homogeneity_score, completeness_score, classification_report, silhouette_samples\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T03:51:03.517808Z",
     "start_time": "2020-06-05T03:50:26.736890Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run(lr=0.0001, batch_size=1000, shuffle=True)\n",
      "/Users/petersontylerd/.pyenv/versions/main37/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "Run(lr=0.0001, batch_size=1000, shuffle=False)\n"
     ]
    }
   ],
   "source": [
    "!python run_executor.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-05T02:58:48.025946Z",
     "start_time": "2020-06-05T02:58:47.965516Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>run</th>\n",
       "      <th>run_duration</th>\n",
       "      <th>epoch</th>\n",
       "      <th>epoch_start_time</th>\n",
       "      <th>epoch_end_time</th>\n",
       "      <th>epoch_duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>shuffle</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>train_precision_macro</th>\n",
       "      <th>train_precision_micro</th>\n",
       "      <th>train_recall_macro</th>\n",
       "      <th>train_recall_micro</th>\n",
       "      <th>train_f1_macro</th>\n",
       "      <th>train_f1_micro</th>\n",
       "      <th>train_number_correct</th>\n",
       "      <th>validation_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.690065</td>\n",
       "      <td>1</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.689918</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>22843.502760</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>0.482754</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>0.263362</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>0.227407</td>\n",
       "      <td>0.2725</td>\n",
       "      <td>2725</td>\n",
       "      <td>0.4589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6.936063</td>\n",
       "      <td>2</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.082987</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>20455.504179</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.550898</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.566559</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>0.468061</td>\n",
       "      <td>0.5875</td>\n",
       "      <td>5875</td>\n",
       "      <td>0.5541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.738333</td>\n",
       "      <td>3</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.639118</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>11383.752644</td>\n",
       "      <td>0.6678</td>\n",
       "      <td>0.652460</td>\n",
       "      <td>0.6678</td>\n",
       "      <td>0.651768</td>\n",
       "      <td>0.6678</td>\n",
       "      <td>0.619779</td>\n",
       "      <td>0.6678</td>\n",
       "      <td>6678</td>\n",
       "      <td>0.6849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>13.988242</td>\n",
       "      <td>4</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.086761</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>6984.959304</td>\n",
       "      <td>0.7615</td>\n",
       "      <td>0.749408</td>\n",
       "      <td>0.7615</td>\n",
       "      <td>0.752138</td>\n",
       "      <td>0.7615</td>\n",
       "      <td>0.748780</td>\n",
       "      <td>0.7615</td>\n",
       "      <td>7615</td>\n",
       "      <td>0.7623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>17.230053</td>\n",
       "      <td>5</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.082521</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>True</td>\n",
       "      <td>5776.385158</td>\n",
       "      <td>0.8211</td>\n",
       "      <td>0.815850</td>\n",
       "      <td>0.8211</td>\n",
       "      <td>0.815641</td>\n",
       "      <td>0.8211</td>\n",
       "      <td>0.815389</td>\n",
       "      <td>0.8211</td>\n",
       "      <td>8211</td>\n",
       "      <td>0.8196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.109322</td>\n",
       "      <td>1</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.100131</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>22839.505911</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.520075</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.313131</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>0.281778</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>3220</td>\n",
       "      <td>0.2297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6.437784</td>\n",
       "      <td>2</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.170789</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>20450.985074</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.582517</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.461011</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>0.393082</td>\n",
       "      <td>0.4809</td>\n",
       "      <td>4809</td>\n",
       "      <td>0.4579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>10.267249</td>\n",
       "      <td>3</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.666797</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>11911.658823</td>\n",
       "      <td>0.6506</td>\n",
       "      <td>0.650778</td>\n",
       "      <td>0.6506</td>\n",
       "      <td>0.634881</td>\n",
       "      <td>0.6506</td>\n",
       "      <td>0.612240</td>\n",
       "      <td>0.6506</td>\n",
       "      <td>6506</td>\n",
       "      <td>0.6693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>13.621993</td>\n",
       "      <td>4</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.191859</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>7108.217835</td>\n",
       "      <td>0.7663</td>\n",
       "      <td>0.758285</td>\n",
       "      <td>0.7663</td>\n",
       "      <td>0.758852</td>\n",
       "      <td>0.7663</td>\n",
       "      <td>0.756734</td>\n",
       "      <td>0.7663</td>\n",
       "      <td>7663</td>\n",
       "      <td>0.7628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>17.044001</td>\n",
       "      <td>5</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>1.591326e+09</td>\n",
       "      <td>3.258657</td>\n",
       "      <td>0.0001</td>\n",
       "      <td>1000</td>\n",
       "      <td>False</td>\n",
       "      <td>5558.384448</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>0.822581</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>0.822421</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>0.822326</td>\n",
       "      <td>0.8271</td>\n",
       "      <td>8271</td>\n",
       "      <td>0.7923</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  run  run_duration  epoch  epoch_start_time  epoch_end_time  \\\n",
       "0           0    1      3.690065      1      1.591326e+09    1.591326e+09   \n",
       "1           1    1      6.936063      2      1.591326e+09    1.591326e+09   \n",
       "2           2    1     10.738333      3      1.591326e+09    1.591326e+09   \n",
       "3           3    1     13.988242      4      1.591326e+09    1.591326e+09   \n",
       "4           4    1     17.230053      5      1.591326e+09    1.591326e+09   \n",
       "5           0    2      3.109322      1      1.591326e+09    1.591326e+09   \n",
       "6           1    2      6.437784      2      1.591326e+09    1.591326e+09   \n",
       "7           2    2     10.267249      3      1.591326e+09    1.591326e+09   \n",
       "8           3    2     13.621993      4      1.591326e+09    1.591326e+09   \n",
       "9           4    2     17.044001      5      1.591326e+09    1.591326e+09   \n",
       "\n",
       "   epoch_duration      lr  batch_size  shuffle    train_loss  train_accuracy  \\\n",
       "0        3.689918  0.0001        1000     True  22843.502760          0.2725   \n",
       "1        3.082987  0.0001        1000     True  20455.504179          0.5875   \n",
       "2        3.639118  0.0001        1000     True  11383.752644          0.6678   \n",
       "3        3.086761  0.0001        1000     True   6984.959304          0.7615   \n",
       "4        3.082521  0.0001        1000     True   5776.385158          0.8211   \n",
       "5        3.100131  0.0001        1000    False  22839.505911          0.3220   \n",
       "6        3.170789  0.0001        1000    False  20450.985074          0.4809   \n",
       "7        3.666797  0.0001        1000    False  11911.658823          0.6506   \n",
       "8        3.191859  0.0001        1000    False   7108.217835          0.7663   \n",
       "9        3.258657  0.0001        1000    False   5558.384448          0.8271   \n",
       "\n",
       "   train_precision_macro  train_precision_micro  train_recall_macro  \\\n",
       "0               0.482754                 0.2725            0.263362   \n",
       "1               0.550898                 0.5875            0.566559   \n",
       "2               0.652460                 0.6678            0.651768   \n",
       "3               0.749408                 0.7615            0.752138   \n",
       "4               0.815850                 0.8211            0.815641   \n",
       "5               0.520075                 0.3220            0.313131   \n",
       "6               0.582517                 0.4809            0.461011   \n",
       "7               0.650778                 0.6506            0.634881   \n",
       "8               0.758285                 0.7663            0.758852   \n",
       "9               0.822581                 0.8271            0.822421   \n",
       "\n",
       "   train_recall_micro  train_f1_macro  train_f1_micro  train_number_correct  \\\n",
       "0              0.2725        0.227407          0.2725                  2725   \n",
       "1              0.5875        0.468061          0.5875                  5875   \n",
       "2              0.6678        0.619779          0.6678                  6678   \n",
       "3              0.7615        0.748780          0.7615                  7615   \n",
       "4              0.8211        0.815389          0.8211                  8211   \n",
       "5              0.3220        0.281778          0.3220                  3220   \n",
       "6              0.4809        0.393082          0.4809                  4809   \n",
       "7              0.6506        0.612240          0.6506                  6506   \n",
       "8              0.7663        0.756734          0.7663                  7663   \n",
       "9              0.8271        0.822326          0.8271                  8271   \n",
       "\n",
       "   validation_accuracy  \n",
       "0               0.4589  \n",
       "1               0.5541  \n",
       "2               0.6849  \n",
       "3               0.7623  \n",
       "4               0.8196  \n",
       "5               0.2297  \n",
       "6               0.4579  \n",
       "7               0.6693  \n",
       "8               0.7628  \n",
       "9               0.7923  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = \"/Users/petersontylerd/pytorch_experiments/20200604_2212_FCNet/tensborboard_logs/events.out.tfevents.1591326729.Tylers-iMac.attlocal.net.5648.0\"\n",
    "pd.read_csv(\"/Users/petersontylerd/pytorch_experiments/20200604_2157_FCNet/raw_logs/results.csv\").iloc[:20, :20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pytorch_runs\n",
    "    - YYYYMMDD_HHMMS_NAME\n",
    "        - model\n",
    "            - best_model.pkl\n",
    "        - tensorboard\n",
    "        - images\n",
    "        - logs\n",
    "            - results_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Data load functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:54.836363Z",
     "start_time": "2020-05-24T15:17:54.825397Z"
    },
    "code_folding": [],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Imaging functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:54.844564Z",
     "start_time": "2020-05-24T15:17:54.839947Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def image_sample(inp, figsize=(20,20)):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.figure(figsize=figsize)\n",
    "    plt.imshow(\n",
    "        inp,\n",
    "        interpolation=\"nearest\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:54.850650Z",
     "start_time": "2020-05-24T15:17:54.847843Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# plot sample image\n",
    "def plot_sample(image):\n",
    "    plt.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:38:50.659777Z",
     "start_time": "2020-05-20T14:38:50.656277Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "## Load training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:54.878617Z",
     "start_time": "2020-05-24T15:17:54.853036Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# load source files\n",
    "X_train, y_train = load_mnist(\n",
    "    path=os.path.join(os.environ[\"HOME\"], \"s3buckets\", \"mnist\"),\n",
    "    kind=\"train\"\n",
    ")\n",
    "\n",
    "# transformation instructions\n",
    "norm_mean = [0.1307]\n",
    "norm_std = [0.3801]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        norm_mean,\n",
    "        norm_std\n",
    "    ),\n",
    "])\n",
    "\n",
    "# load data into Pytorch Dataset\n",
    "train_data = MNISTDataset(\n",
    "    images=X_train[:6000,:],\n",
    "    targets=y_train[:6000],\n",
    "    transform=train_transform,\n",
    ")\n",
    "\n",
    "# create Pytorch DataLoader\n",
    "train_data_loader = torch.utils.data.DataLoader(\n",
    "    train_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### review samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:55.184146Z",
     "start_time": "2020-05-24T15:17:54.881072Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# visualize image batch grid\n",
    "inputs, classes = next(iter(train_data_loader))\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "image_sample(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:55.314082Z",
     "start_time": "2020-05-24T15:17:55.186189Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "sample = iter(train_data_loader.dataset.images)\n",
    "plot_sample(next(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T14:38:50.659777Z",
     "start_time": "2020-05-20T14:38:50.656277Z"
    }
   },
   "source": [
    "## Load validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:55.325820Z",
     "start_time": "2020-05-24T15:17:55.317994Z"
    }
   },
   "outputs": [],
   "source": [
    "# load source files\n",
    "X_valid, y_valid = load_mnist(\n",
    "    path=os.path.join(os.environ[\"HOME\"], \"s3buckets\", \"mnist\"),\n",
    "    kind=\"t10k\"\n",
    ")\n",
    "\n",
    "# transformation instructions\n",
    "norm_mean = [0.1307]\n",
    "norm_std = [0.3801]\n",
    "\n",
    "validation_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        norm_mean,\n",
    "        norm_std\n",
    "    )\n",
    "])\n",
    "\n",
    "# load data into Pytorch dataset\n",
    "validation_data = MNISTDataset(\n",
    "    images=X_valid,\n",
    "    targets=y_valid,\n",
    "    transform=validation_transform,\n",
    ")\n",
    "\n",
    "# create Pytorch DataLoader\n",
    "validation_data_loader = torch.utils.data.DataLoader(\n",
    "    validation_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    # sampler=weighted_sampler\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:55.620573Z",
     "start_time": "2020-05-24T15:17:55.328182Z"
    }
   },
   "outputs": [],
   "source": [
    "# visualize image batch grid\n",
    "inputs, classes = next(iter(validation_data_loader))\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "image_sample(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:17:55.752027Z",
     "start_time": "2020-05-24T15:17:55.622658Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = iter(validation_data_loader.dataset.images)\n",
    "plot_sample(next(sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RunExecutor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-24T15:47:49.302701Z",
     "start_time": "2020-05-24T15:47:47.035100Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Parameter setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T03:17:33.899199Z",
     "start_time": "2020-05-23T03:17:33.893112Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# set input kwargs as object attributes\n",
    "class ParamConfig:  \n",
    "    def __init__(self, **kwargs):\n",
    "        for key, value in kwargs.items():\n",
    "            setattr(self, key, value)\n",
    "\n",
    "# configure all necessary parameters\n",
    "model_params = ParamConfig(\n",
    "    model = FCNet,\n",
    "    model_name = \"FCNet\",\n",
    "#     model_object_dir = \"/content/drive/model_objects/20191202_1622_VGG16\",\n",
    "    model_object_dir = None,\n",
    "    optimizer = torch.optim.Adam,\n",
    "    criterion = F.cross_entropy,\n",
    "#     criterion = F.nll_loss,\n",
    "    train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=128, shuffle=True),\n",
    "    valid_data_loader = torch.utils.data.DataLoader(valid_data, batch_size=128, shuffle=True),\n",
    "    cuda = True if torch.cuda.is_available() else False,\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"),\n",
    "    seed = 0,\n",
    "    lr = 0.001,\n",
    "    epochs = 50,\n",
    "    tensorboard_files = False,\n",
    "    verbose = True,\n",
    "    save_model_objects=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T04:00:56.885115Z",
     "start_time": "2020-05-23T04:00:56.880466Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class ClassificationBoard():\n",
    "    \n",
    "    def __init__(self, root_log_dir, experiment_name):\n",
    "        self.experiment_name = experiment_name\n",
    "        self.root_log_dir = root_log_dir        \n",
    "        self.log_dir = os.path.join(self.root_log_dir, self.experiment_name, datetime.today().strftime('%Y%m%d_%H%M'))\n",
    "        self.summary_writer = SummaryWriter(self.log_dir)\n",
    "        \n",
    "        \n",
    "    def log_scalars(self, scalars):\n",
    "        \n",
    "        for tag, value in scalars.items():\n",
    "            self.summary_writer.add_scalar(tag, value, step+1)\n",
    "\n",
    "test = ClassificationBoard(log_dir=tensorboard_log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-23T03:54:18.468965Z",
     "start_time": "2020-05-23T03:54:18.464675Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "os.path.isdir(\"tensorboard_logdir\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "if (step+1) % 100 == 0:\n",
    "    print ('Step [{}/{}], Loss: {:.4f}, Acc: {:.2f}' \n",
    "           .format(step+1, total_step, loss.item(), accuracy.item()))\n",
    "\n",
    "    # ================================================================== #\n",
    "    #                        Tensorboard Logging                         #\n",
    "    # ================================================================== #\n",
    "\n",
    "    # 1. Log scalar values (scalar summary)\n",
    "    info = { 'loss': loss.item(), 'accuracy': accuracy.item() }\n",
    "\n",
    "    for tag, value in info.items():\n",
    "        logger.scalar_summary(tag, value, step+1)\n",
    "\n",
    "    # 2. Log values and gradients of the parameters (histogram summary)\n",
    "    for tag, value in model.named_parameters():\n",
    "        tag = tag.replace('.', '/')\n",
    "        logger.histo_summary(tag, value.data.cpu().numpy(), step+1)\n",
    "        logger.histo_summary(tag+'/grad', value.grad.data.cpu().numpy(), step+1)\n",
    "\n",
    "    # 3. Log training images (image summary)\n",
    "    info = { 'images': images.view(-1, 28, 28)[:10].cpu().numpy() }\n",
    "\n",
    "    for tag, images in info.items():\n",
    "        logger.image_summary(tag, images, step+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Training procedure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:25:28.513553Z",
     "start_time": "2020-05-20T15:25:28.494472Z"
    },
    "code_folding": [
     2,
     41,
     72,
     219
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PyTorchTrainer:\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        # random seed settings\n",
    "        self.seed = config.seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.verbose = config.verbose\n",
    "\n",
    "        # data loaders\n",
    "        self.train_data_loader = config.train_data_loader\n",
    "        self.validation_data_loader = config.validation_data_loader\n",
    "\n",
    "        ## model object creation and device assignment\n",
    "        self.device = config.device\n",
    "\n",
    "        # if passing in the name of model Class object\n",
    "        if isinstance(config.model, type):\n",
    "            self.model = config.model().to(self.device)\n",
    "        # if model is already instantiated, or if transfer learning model is used\n",
    "        else:\n",
    "            self.model = config.model.to(self.device)\n",
    "\n",
    "        # name to use when saving model state\n",
    "        if config.model_name is not None:\n",
    "            self.model_name = config.model_name\n",
    "        else:\n",
    "            self.model_name = \"untitled\"\n",
    "\n",
    "        # model training settings\n",
    "        self.lr = config.lr\n",
    "        self.epochs = config.epochs\n",
    "        self.optimizer = config.optimizer(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = config.criterion\n",
    "\n",
    "        self.n_epochs_stop = 5\n",
    "        self.min_val_loss = np.inf\n",
    "        self.epochs_no_improve = 0\n",
    "\n",
    "        ## load previous state\n",
    "        # use checkpoint to load model state and associated objects\n",
    "        if config.model_object_dir is not None:\n",
    "            print(\">>> Resuming training...\")\n",
    "            self.model_object_dir = config.model_object_dir\n",
    "\n",
    "            # establish directory\n",
    "            self.model_dir = os.path.join(self.model_object_dir, \"models\")\n",
    "            self.object_dir = os.path.join(self.model_object_dir, \"objects\")\n",
    "            self.log_dir = os.path.join(self.model_object_dir, \"logs\")\n",
    "            self.log_train_dir = os.path.join(self.model_object_dir, \"logs\",\"train\")\n",
    "            self.log_validation_dir = os.path.join(self.model_object_dir, \"logs\",\"validation\")\n",
    "            \n",
    "            # load model\n",
    "            self.model.load_state_dict(torch.load(os.path.join(self.model_dir, os.listdir(self.model_dir)[0])))\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model_name = os.listdir(self.model_dir)[0].split(\".\")[0]\n",
    "            \n",
    "            # load statistics objects\n",
    "            self.running_avg_train_f1 = torch.load(os.path.join(self.object_dir, \"running_avg_train_f1.pt\"))\n",
    "            self.running_avg_train_precision = torch.load(os.path.join(self.object_dir, \"running_avg_train_precision.pt\"))\n",
    "            self.running_avg_train_recall = torch.load(os.path.join(self.object_dir, \"running_avg_train_recall.pt\"))\n",
    "            self.running_avg_train_accuracy = torch.load(os.path.join(self.object_dir, \"running_avg_train_accuracy.pt\"))\n",
    "            self.running_avg_train_loss = torch.load(os.path.join(self.object_dir, \"running_avg_train_loss.pt\"))\n",
    "            \n",
    "            self.running_avg_validation_f1 = torch.load(os.path.join(self.object_dir, \"running_avg_validation_f1.pt\"))\n",
    "            self.running_avg_validation_precision = torch.load(os.path.join(self.object_dir, \"running_avg_validation_precision.pt\"))\n",
    "            self.running_avg_validation_recall = torch.load(os.path.join(self.object_dir, \"running_avg_validation_recall.pt\"))\n",
    "            self.running_avg_validation_accuracy = torch.load(os.path.join(self.object_dir, \"running_avg_validation_accuracy.pt\"))\n",
    "            self.running_avg_validation_loss = torch.load(os.path.join(self.object_dir, \"running_avg_validation_loss.pt\"))\n",
    "\n",
    "            self.globaliter = torch.load(os.path.join(self.object_dir, \"globaliter.pt\"))\n",
    "    \n",
    "        else:\n",
    "            # directory tree for storing model attributes\n",
    "            current = datetime.today().strftime('%Y%m%d_%H%M') + \"_\" + self.model_name\n",
    "            \n",
    "            self.model_object_dir = os.path.join(os.getcwd(), \"model_objects\", current)\n",
    "            self.model_dir = os.path.join(self.model_object_dir, \"models\")\n",
    "            self.object_dir = os.path.join(self.model_object_dir, \"objects\")\n",
    "            self.log_dir = os.path.join(self.model_object_dir, \"logs\")\n",
    "            self.log_train_dir = os.path.join(self.model_object_dir, \"logs\",\"train\")\n",
    "            self.log_validation_dir = os.path.join(self.model_object_dir, \"logs\",\"validation\")\n",
    "            \n",
    "            os.makedirs(self.model_object_dir, exist_ok=True)\n",
    "            os.makedirs(self.model_dir, exist_ok=True)\n",
    "            os.makedirs(self.object_dir, exist_ok=True)\n",
    "            os.makedirs(self.log_dir, exist_ok=True)\n",
    "            os.makedirs(self.log_train_dir, exist_ok=True)\n",
    "            os.makedirs(self.log_validation_dir, exist_ok=True)\n",
    "            \n",
    "            self.globaliter = 0\n",
    "\n",
    "        # tensorboard\n",
    "        self.tensorboard_files = config.tensorboard_files\n",
    "        if self.tensorboard_files:\n",
    "            self.train_summary_writer = SummaryWriter(self.log_train_dir)\n",
    "            self.validation_summary_writer = SummaryWriter(self.log_validation_dir)\n",
    "        else:\n",
    "            self.train_summary_writer = None\n",
    "            self.validation_summary_writer = None\n",
    "            \n",
    "        self.beginning_time = time.time()\n",
    "\n",
    "    def train(self, epoch):\n",
    "        epoch_preds = []\n",
    "        epoch_targets = []\n",
    "\n",
    "        epoch_f1 = []\n",
    "        epoch_precision = []\n",
    "        epoch_recall = []\n",
    "        epoch_accuracy = []\n",
    "        epoch_loss = []\n",
    "\n",
    "        self.globaliter += 1\n",
    "        epoch_beginning_time = time.time()\n",
    "        \n",
    "        # sample batch number for data capture\n",
    "        num_batches = np.floor(len(self.train_data_loader.dataset.image_paths) / self.train_data_loader.batch_size)\n",
    "        sample_batch_idx = np.random.randint(0, num_batches)\n",
    "\n",
    "        self.model.train()\n",
    "        print(\"*\" * 100)\n",
    "        for batch_idx, (data, target) in enumerate(self.train_data_loader):\n",
    "            batch_beginning_time = time.time()\n",
    "\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "            output = self.model(data)\n",
    "            train_loss = self.criterion(output, target)\n",
    "            epoch_loss.append(train_loss.item())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            #Metrics\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            epoch_preds = epoch_preds + pred.detach().cpu().numpy().tolist()\n",
    "            epoch_targets = epoch_targets + target.detach().cpu().numpy().tolist()\n",
    "\n",
    "            metric_f1 = f1_score(epoch_targets, epoch_preds)\n",
    "            epoch_f1.append(metric_f1)\n",
    "\n",
    "            metric_precision = precision_score(epoch_targets, epoch_preds)\n",
    "            epoch_precision.append(metric_precision)\n",
    "\n",
    "            metric_recall = recall_score(epoch_targets, epoch_preds)\n",
    "            epoch_recall.append(metric_recall)\n",
    "\n",
    "            metric_accuracy = accuracy_score(epoch_targets, epoch_preds)\n",
    "            epoch_accuracy.append(metric_accuracy)\n",
    "\n",
    "            # print progress report\n",
    "            if self.verbose:\n",
    "                if batch_idx % 50 == 0 and batch_idx > 0:\n",
    "                    print(\"\\nTrain epoch: {} | Batch: {} | [Processed {}/{} ({:.0f}%)]\\n\\tLoss: {:.6f} | F1: {:.6f} | Precision: {:.6f} | Recall: {:.6f} | Accuracy: {:.6f}\".format(\n",
    "                        epoch, batch_idx, len(epoch_preds), len(self.train_data_loader.dataset),\n",
    "                        100. * len(epoch_preds) / len(self.train_data_loader.dataset), train_loss.item(), metric_f1,\n",
    "                        metric_precision, metric_recall, metric_accuracy))\n",
    "                    print(\"\\tBatch time elapsed: {}\\n\".format(self.train_timer(batch_beginning_time, time.time())))\n",
    "                    print(\"\\n\" + \"*\" * 10)\n",
    "\n",
    "            # # image batch sample\n",
    "            # if batch_idx == sample_batch_idx:\n",
    "            #     image_grid = torchvision.utils.make_grid(data.cpu())\n",
    "            #     self.train_summary_writer.add_image('train/Sample batch', image_grid, global_step=self.globaliter)\n",
    "            \n",
    "        # mark epoch end timestamp\n",
    "        epoch_ending_time = time.time()\n",
    "\n",
    "        try:\n",
    "            self.running_avg_train_f1.append((sum(epoch_f1) / len(epoch_f1)))\n",
    "            self.running_avg_train_precision.append((sum(epoch_precision) / len(epoch_precision)))\n",
    "            self.running_avg_train_recall.append((sum(epoch_recall) / len(epoch_recall)))\n",
    "            self.running_avg_train_accuracy.append((sum(epoch_accuracy) / len(epoch_accuracy)))\n",
    "            self.running_avg_train_loss.append((sum(epoch_loss) / len(epoch_loss)))\n",
    "\n",
    "            torch.save(self.running_avg_train_f1, os.path.join(self.object_dir, \"running_avg_train_f1.pt\"))\n",
    "            torch.save(self.running_avg_train_precision, os.path.join(self.object_dir, \"running_avg_train_precision.pt\"))\n",
    "            torch.save(self.running_avg_train_recall, os.path.join(self.object_dir, \"running_avg_train_recall.pt\"))\n",
    "            torch.save(self.running_avg_train_accuracy, os.path.join(self.object_dir, \"running_avg_train_accuracy.pt\"))\n",
    "            torch.save(self.running_avg_train_loss, os.path.join(self.object_dir, \"running_avg_train_loss.pt\"))\n",
    "\n",
    "            # tensorboard\n",
    "            if self.tensorboard_files:\n",
    "                self.train_summary_writer.add_scalar('train/F1', self.running_avg_train_f1[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Precision', self.running_avg_train_precision[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Recall', self.running_avg_train_recall[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Accuracy', self.running_avg_train_accuracy[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Loss', self.running_avg_train_loss[-1], global_step=self.globaliter)\n",
    "\n",
    "                self.train_summary_writer.add_scalar('F1', self.running_avg_train_f1[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Precision', self.running_avg_train_precision[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Recall', self.running_avg_train_recall[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Accuracy', self.running_avg_train_accuracy[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Loss', self.running_avg_train_loss[-1], global_step=self.globaliter)\n",
    "                                \n",
    "                self.train_summary_writer.flush()\n",
    "\n",
    "        except AttributeError:\n",
    "            self.running_avg_train_f1 = [(sum(epoch_f1) / len(epoch_f1))]\n",
    "            self.running_avg_train_precision = [(sum(epoch_precision) / len(epoch_precision))]\n",
    "            self.running_avg_train_recall = [(sum(epoch_recall) / len(epoch_recall))]\n",
    "            self.running_avg_train_accuracy = [(sum(epoch_accuracy) / len(epoch_accuracy))]\n",
    "            self.running_avg_train_loss = [(sum(epoch_loss) / len(epoch_loss))]\n",
    "\n",
    "        # print progress report\n",
    "        if self.verbose:\n",
    "            print(\"*\" * 10 + \"\\n\")\n",
    "            print(\"Train epoch: {} \\n\\tLoss: {:.6f} | F1: {:.6f} | Precision: {:.6f} | Recall: {:.6f} | Accuracy: {:.6f}\\n\".format(\n",
    "                        epoch, self.running_avg_train_loss[-1], self.running_avg_train_f1[-1],\n",
    "                        self.running_avg_train_precision[-1], self.running_avg_train_recall[-1], self.running_avg_train_accuracy[-1]))\n",
    "            print(\"\\tEpoch time elapsed: {}\".format(self.train_timer(epoch_beginning_time, epoch_ending_time)))\n",
    "            print(\"\\tTotal time elapsed: {}\".format(self.train_timer(self.beginning_time, time.time())))\n",
    "        \n",
    "        # capture globaliter\n",
    "        torch.save(self.globaliter, os.path.join(self.object_dir, \"globaliter.pt\"))\n",
    "\n",
    "    def validation(self, epoch):\n",
    "        epoch_preds = []\n",
    "        epoch_targets = []\n",
    "\n",
    "        epoch_f1 = []\n",
    "        epoch_precision = []\n",
    "        epoch_recall = []\n",
    "        epoch_accuracy = []\n",
    "        epoch_loss = []\n",
    "        \n",
    "        # sample batch number for data capture\n",
    "        num_batches = np.floor(len(self.validation_data_loader.dataset.image_paths) / self.validation_data_loader.batch_size)\n",
    "        sample_batch_idx = np.random.randint(0, num_batches)\n",
    "\n",
    "        # turn off gradients\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(self.validation_data_loader):\n",
    "                # reshape data as needed and send data to GPU if available\n",
    "                data = data.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "\n",
    "                # generate predictions\n",
    "                output = self.model(data)\n",
    "\n",
    "                validation_loss = self.criterion(output, target)\n",
    "                epoch_loss.append(validation_loss.item())\n",
    "\n",
    "                #Metrics\n",
    "                _, pred = torch.max(output, dim=1)\n",
    "                epoch_preds = epoch_preds + pred.detach().cpu().numpy().tolist()\n",
    "                epoch_targets = epoch_targets + target.detach().cpu().numpy().tolist()\n",
    "\n",
    "                metric_f1 = f1_score(epoch_targets, epoch_preds)\n",
    "                epoch_f1.append(metric_f1)\n",
    "\n",
    "                metric_precision = precision_score(epoch_targets, epoch_preds)\n",
    "                epoch_precision.append(metric_precision)\n",
    "\n",
    "                metric_recall = recall_score(epoch_targets, epoch_preds)\n",
    "                epoch_recall.append(metric_recall)\n",
    "\n",
    "                metric_accuracy = accuracy_score(epoch_targets, epoch_preds)\n",
    "                epoch_accuracy.append(metric_accuracy)\n",
    "            \n",
    "            # #\n",
    "            # if batch_idx == sample_batch_idx:\n",
    "            #     image_grid = torchvision.utils.make_grid(data.cpu())\n",
    "            #     self.validation_summary_writer.add_image('validation/Sample batch', image_grid, global_step=self.globaliter)\n",
    "                \n",
    "            # \n",
    "            try:\n",
    "                self.running_avg_validation_f1.append((sum(epoch_f1) / len(epoch_f1)))\n",
    "                self.running_avg_validation_precision.append((sum(epoch_precision) / len(epoch_precision)))\n",
    "                self.running_avg_validation_recall.append((sum(epoch_recall) / len(epoch_recall)))\n",
    "                self.running_avg_validation_accuracy.append((sum(epoch_accuracy) / len(epoch_accuracy)))\n",
    "                self.running_avg_validation_loss.append((sum(epoch_loss) / len(epoch_loss)))\n",
    "\n",
    "                torch.save(self.running_avg_validation_f1, os.path.join(self.object_dir, \"running_avg_validation_f1.pt\"))\n",
    "                torch.save(self.running_avg_validation_precision, os.path.join(self.object_dir, \"running_avg_validation_precision.pt\"))\n",
    "                torch.save(self.running_avg_validation_recall, os.path.join(self.object_dir, \"running_avg_validation_recall.pt\"))\n",
    "                torch.save(self.running_avg_validation_accuracy, os.path.join(self.object_dir, \"running_avg_validation_accuracy.pt\"))\n",
    "                torch.save(self.running_avg_validation_loss, os.path.join(self.object_dir, \"running_avg_validation_loss.pt\"))\n",
    "\n",
    "                # tensorboard\n",
    "                if self.tensorboard_files:\n",
    "                    # validation panel - one scalar per metric per plot\n",
    "                    self.validation_summary_writer.add_scalar('validation/F1', self.running_avg_validation_f1[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Precision', self.running_avg_validation_precision[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Recall', self.running_avg_validation_recall[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Accuracy', self.running_avg_validation_accuracy[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Loss', self.running_avg_validation_loss[-1], global_step=self.globaliter)\n",
    "\n",
    "                    # metric-specific plots\n",
    "                    self.validation_summary_writer.add_scalar('F1', self.running_avg_validation_f1[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Precision', self.running_avg_validation_precision[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Recall', self.running_avg_validation_recall[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Accuracy', self.running_avg_validation_accuracy[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Loss', self.running_avg_validation_loss[-1], global_step=self.globaliter)\n",
    "                    \n",
    "                    self.validation_summary_writer.flush()\n",
    "\n",
    "            # create statistics object and continue\n",
    "            except AttributeError:\n",
    "                self.running_avg_validation_f1 = [(sum(epoch_f1) / len(epoch_f1))]\n",
    "                self.running_avg_validation_precision = [(sum(epoch_precision) / len(epoch_precision))]\n",
    "                self.running_avg_validation_recall = [(sum(epoch_recall) / len(epoch_recall))]\n",
    "                self.running_avg_validation_accuracy = [(sum(epoch_accuracy) / len(epoch_accuracy))]\n",
    "                self.running_avg_validation_loss = [(sum(epoch_loss) / len(epoch_loss))]\n",
    "                \n",
    "            # print progress report\n",
    "            if self.verbose:\n",
    "                print(\"\\nValidation epoch: {} \\n\\tLoss: {:.6f} | F1: {:.6f} | Precision: {:.6f} | Recall: {:.6f} | Accuracy: {:.6f}\\n\".format(\n",
    "                        epoch, self.running_avg_validation_loss[-1], self.running_avg_validation_f1[-1],\n",
    "                        self.running_avg_validation_precision[-1], self.running_avg_validation_recall[-1], self.running_avg_validation_accuracy[-1]))\n",
    "        \n",
    "            # early stopping\n",
    "            if self.running_avg_validation_loss[-1] < self.min_val_loss:\n",
    "                # Save the model checkpoint\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.model_dir, \"{}.pt\".format(self.model_name)))\n",
    "                self.epochs_no_improve = 0\n",
    "                self.min_val_loss = self.running_avg_validation_loss[-1]\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(\">>> Improved - saving model\\n\\n\\n\")\n",
    "\n",
    "            else:\n",
    "                self.epochs_no_improve += 1\n",
    "                if self.verbose:\n",
    "                    print(\">>> No improvement - {} consecutive epochs\\n\\n\\n\".format(self.epochs_no_improve))\n",
    "                if self.epochs_no_improve == self.n_epochs_stop:\n",
    "                    if self.verbose:\n",
    "                        print(\"\\n!!! Early stopping - {} epochs without improvement\\n\".format(self.n_epochs_stop))\n",
    "                self.running_avg_validation_loss = []\n",
    "                    \n",
    "    def train_timer(self, start, end):\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Execute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-20T15:29:17.887861Z",
     "start_time": "2020-05-20T15:25:30.758839Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "### fit model\n",
    "# instantiate model object\n",
    "trainer = PyTorchTrainer(config=model_params)\n",
    "\n",
    "# iterate fitting procedure over specified epoch count\n",
    "for epoch in range(1, trainer.epochs + 1):\n",
    "    trainer.train(epoch)\n",
    "    trainer.validation(epoch)\n",
    "trainer.train_summary_writer.close()\n",
    "trainer.validation_summary_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class PyTorchTrainer:\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        # random seed settings\n",
    "        self.seed = config.seed\n",
    "        torch.manual_seed(self.seed)\n",
    "        self.verbose = config.verbose\n",
    "\n",
    "        # data loaders\n",
    "        self.train_data_loader = config.train_data_loader\n",
    "        self.validation_data_loader = config.validation_data_loader\n",
    "\n",
    "        ## model object creation and device assignment\n",
    "        self.device = config.device\n",
    "\n",
    "        # if passing in the name of model Class object\n",
    "        if isinstance(config.model, type):\n",
    "            self.model = config.model().to(self.device)\n",
    "        # if model is already instantiated, or if transfer learning model is used\n",
    "        else:\n",
    "            self.model = config.model.to(self.device)\n",
    "\n",
    "        # name to use when saving model state\n",
    "        if config.model_name is not None:\n",
    "            self.model_name = config.model_name\n",
    "        else:\n",
    "            self.model_name = \"untitled\"\n",
    "\n",
    "        # model training settings\n",
    "        self.lr = config.lr\n",
    "        self.epochs = config.epochs\n",
    "        self.optimizer = config.optimizer(self.model.parameters(), lr=self.lr)\n",
    "        self.criterion = config.criterion\n",
    "\n",
    "        self.n_epochs_stop = 5\n",
    "        self.min_val_loss = np.inf\n",
    "        self.epochs_no_improve = 0\n",
    "\n",
    "        ## load previous state\n",
    "        # use checkpoint to load model state and associated objects\n",
    "        if config.model_object_dir is not None:\n",
    "            print(\">>> Resuming training...\")\n",
    "            self.model_object_dir = config.model_object_dir\n",
    "\n",
    "            # establish directory\n",
    "            self.model_dir = os.path.join(self.model_object_dir, \"models\")\n",
    "            self.object_dir = os.path.join(self.model_object_dir, \"objects\")\n",
    "            self.log_dir = os.path.join(self.model_object_dir, \"logs\")\n",
    "            self.log_train_dir = os.path.join(self.model_object_dir, \"logs\",\"train\")\n",
    "            self.log_validation_dir = os.path.join(self.model_object_dir, \"logs\",\"validation\")\n",
    "            \n",
    "            # load model\n",
    "            self.model.load_state_dict(torch.load(os.path.join(self.model_dir, os.listdir(self.model_dir)[0])))\n",
    "            self.model = self.model.to(self.device)\n",
    "            self.model_name = os.listdir(self.model_dir)[0].split(\".\")[0]\n",
    "            \n",
    "            # load statistics objects\n",
    "            self.running_avg_train_f1 = torch.load(os.path.join(self.object_dir, \"running_avg_train_f1.pt\"))\n",
    "            self.running_avg_train_precision = torch.load(os.path.join(self.object_dir, \"running_avg_train_precision.pt\"))\n",
    "            self.running_avg_train_recall = torch.load(os.path.join(self.object_dir, \"running_avg_train_recall.pt\"))\n",
    "            self.running_avg_train_accuracy = torch.load(os.path.join(self.object_dir, \"running_avg_train_accuracy.pt\"))\n",
    "            self.running_avg_train_loss = torch.load(os.path.join(self.object_dir, \"running_avg_train_loss.pt\"))\n",
    "            \n",
    "            self.running_avg_validation_f1 = torch.load(os.path.join(self.object_dir, \"running_avg_validation_f1.pt\"))\n",
    "            self.running_avg_validation_precision = torch.load(os.path.join(self.object_dir, \"running_avg_validation_precision.pt\"))\n",
    "            self.running_avg_validation_recall = torch.load(os.path.join(self.object_dir, \"running_avg_validation_recall.pt\"))\n",
    "            self.running_avg_validation_accuracy = torch.load(os.path.join(self.object_dir, \"running_avg_validation_accuracy.pt\"))\n",
    "            self.running_avg_validation_loss = torch.load(os.path.join(self.object_dir, \"running_avg_validation_loss.pt\"))\n",
    "\n",
    "            self.globaliter = torch.load(os.path.join(self.object_dir, \"globaliter.pt\"))\n",
    "    \n",
    "        else:\n",
    "            # directory tree for storing model attributes\n",
    "            current = datetime.today().strftime('%Y%m%d_%H%M') + \"_\" + self.model_name\n",
    "            \n",
    "            self.model_object_dir = os.path.join(os.getcwd(), \"model_objects\", current)\n",
    "            self.model_dir = os.path.join(self.model_object_dir, \"models\")\n",
    "            self.object_dir = os.path.join(self.model_object_dir, \"objects\")\n",
    "            self.log_dir = os.path.join(self.model_object_dir, \"logs\")\n",
    "            self.log_train_dir = os.path.join(self.model_object_dir, \"logs\",\"train\")\n",
    "            self.log_validation_dir = os.path.join(self.model_object_dir, \"logs\",\"validation\")\n",
    "            \n",
    "            os.makedirs(self.model_object_dir, exist_ok=True)\n",
    "            os.makedirs(self.model_dir, exist_ok=True)\n",
    "            os.makedirs(self.object_dir, exist_ok=True)\n",
    "            os.makedirs(self.log_dir, exist_ok=True)\n",
    "            os.makedirs(self.log_train_dir, exist_ok=True)\n",
    "            os.makedirs(self.log_validation_dir, exist_ok=True)\n",
    "            \n",
    "            self.globaliter = 0\n",
    "\n",
    "        # tensorboard\n",
    "        self.tensorboard_files = config.tensorboard_files\n",
    "        if self.tensorboard_files:\n",
    "            self.train_summary_writer = SummaryWriter(self.log_train_dir)\n",
    "            self.validation_summary_writer = SummaryWriter(self.log_validation_dir)\n",
    "        else:\n",
    "            self.train_summary_writer = None\n",
    "            self.validation_summary_writer = None\n",
    "            \n",
    "        self.beginning_time = time.time()\n",
    "\n",
    "    def train(self, epoch):\n",
    "        epoch_preds = []\n",
    "        epoch_targets = []\n",
    "\n",
    "        epoch_f1 = []\n",
    "        epoch_precision = []\n",
    "        epoch_recall = []\n",
    "        epoch_accuracy = []\n",
    "        epoch_loss = []\n",
    "\n",
    "        self.globaliter += 1\n",
    "        epoch_beginning_time = time.time()\n",
    "        \n",
    "        # sample batch number for data capture\n",
    "        num_batches = np.floor(len(self.train_data_loader.dataset.image_paths) / self.train_data_loader.batch_size)\n",
    "        sample_batch_idx = np.random.randint(0, num_batches)\n",
    "\n",
    "        self.model.train()\n",
    "        print(\"*\" * 100)\n",
    "        for batch_idx, (data, target) in enumerate(self.train_data_loader):\n",
    "            batch_beginning_time = time.time()\n",
    "\n",
    "            data = data.to(self.device)\n",
    "            target = target.to(self.device)\n",
    "\n",
    "            output = self.model(data)\n",
    "            train_loss = self.criterion(output, target)\n",
    "            epoch_loss.append(train_loss.item())\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "            #Metrics\n",
    "            _, pred = torch.max(output, dim=1)\n",
    "            epoch_preds = epoch_preds + pred.detach().cpu().numpy().tolist()\n",
    "            epoch_targets = epoch_targets + target.detach().cpu().numpy().tolist()\n",
    "\n",
    "            metric_f1 = f1_score(epoch_targets, epoch_preds)\n",
    "            epoch_f1.append(metric_f1)\n",
    "\n",
    "            metric_precision = precision_score(epoch_targets, epoch_preds)\n",
    "            epoch_precision.append(metric_precision)\n",
    "\n",
    "            metric_recall = recall_score(epoch_targets, epoch_preds)\n",
    "            epoch_recall.append(metric_recall)\n",
    "\n",
    "            metric_accuracy = accuracy_score(epoch_targets, epoch_preds)\n",
    "            epoch_accuracy.append(metric_accuracy)\n",
    "\n",
    "            # print progress report\n",
    "            if self.verbose:\n",
    "                if batch_idx % 50 == 0 and batch_idx > 0:\n",
    "                    print(\"\\nTrain epoch: {} | Batch: {} | [Processed {}/{} ({:.0f}%)]\\n\\tLoss: {:.6f} | F1: {:.6f} | Precision: {:.6f} | Recall: {:.6f} | Accuracy: {:.6f}\".format(\n",
    "                        epoch, batch_idx, len(epoch_preds), len(self.train_data_loader.dataset),\n",
    "                        100. * len(epoch_preds) / len(self.train_data_loader.dataset), train_loss.item(), metric_f1,\n",
    "                        metric_precision, metric_recall, metric_accuracy))\n",
    "                    print(\"\\tBatch time elapsed: {}\\n\".format(self.train_timer(batch_beginning_time, time.time())))\n",
    "                    print(\"\\n\" + \"*\" * 10)\n",
    "\n",
    "            # # image batch sample\n",
    "            # if batch_idx == sample_batch_idx:\n",
    "            #     image_grid = torchvision.utils.make_grid(data.cpu())\n",
    "            #     self.train_summary_writer.add_image('train/Sample batch', image_grid, global_step=self.globaliter)\n",
    "            \n",
    "        # mark epoch end timestamp\n",
    "        epoch_ending_time = time.time()\n",
    "\n",
    "        try:\n",
    "            self.running_avg_train_f1.append((sum(epoch_f1) / len(epoch_f1)))\n",
    "            self.running_avg_train_precision.append((sum(epoch_precision) / len(epoch_precision)))\n",
    "            self.running_avg_train_recall.append((sum(epoch_recall) / len(epoch_recall)))\n",
    "            self.running_avg_train_accuracy.append((sum(epoch_accuracy) / len(epoch_accuracy)))\n",
    "            self.running_avg_train_loss.append((sum(epoch_loss) / len(epoch_loss)))\n",
    "\n",
    "            torch.save(self.running_avg_train_f1, os.path.join(self.object_dir, \"running_avg_train_f1.pt\"))\n",
    "            torch.save(self.running_avg_train_precision, os.path.join(self.object_dir, \"running_avg_train_precision.pt\"))\n",
    "            torch.save(self.running_avg_train_recall, os.path.join(self.object_dir, \"running_avg_train_recall.pt\"))\n",
    "            torch.save(self.running_avg_train_accuracy, os.path.join(self.object_dir, \"running_avg_train_accuracy.pt\"))\n",
    "            torch.save(self.running_avg_train_loss, os.path.join(self.object_dir, \"running_avg_train_loss.pt\"))\n",
    "\n",
    "            # tensorboard\n",
    "            if self.tensorboard_files:\n",
    "                self.train_summary_writer.add_scalar('train/F1', self.running_avg_train_f1[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Precision', self.running_avg_train_precision[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Recall', self.running_avg_train_recall[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Accuracy', self.running_avg_train_accuracy[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('train/Loss', self.running_avg_train_loss[-1], global_step=self.globaliter)\n",
    "\n",
    "                self.train_summary_writer.add_scalar('F1', self.running_avg_train_f1[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Precision', self.running_avg_train_precision[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Recall', self.running_avg_train_recall[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Accuracy', self.running_avg_train_accuracy[-1], global_step=self.globaliter)\n",
    "                self.train_summary_writer.add_scalar('Loss', self.running_avg_train_loss[-1], global_step=self.globaliter)\n",
    "                                \n",
    "                self.train_summary_writer.flush()\n",
    "\n",
    "        except AttributeError:\n",
    "            self.running_avg_train_f1 = [(sum(epoch_f1) / len(epoch_f1))]\n",
    "            self.running_avg_train_precision = [(sum(epoch_precision) / len(epoch_precision))]\n",
    "            self.running_avg_train_recall = [(sum(epoch_recall) / len(epoch_recall))]\n",
    "            self.running_avg_train_accuracy = [(sum(epoch_accuracy) / len(epoch_accuracy))]\n",
    "            self.running_avg_train_loss = [(sum(epoch_loss) / len(epoch_loss))]\n",
    "\n",
    "        # print progress report\n",
    "        if self.verbose:\n",
    "            print(\"*\" * 10 + \"\\n\")\n",
    "            print(\"Train epoch: {} \\n\\tLoss: {:.6f} | F1: {:.6f} | Precision: {:.6f} | Recall: {:.6f} | Accuracy: {:.6f}\\n\".format(\n",
    "                        epoch, self.running_avg_train_loss[-1], self.running_avg_train_f1[-1],\n",
    "                        self.running_avg_train_precision[-1], self.running_avg_train_recall[-1], self.running_avg_train_accuracy[-1]))\n",
    "            print(\"\\tEpoch time elapsed: {}\".format(self.train_timer(epoch_beginning_time, epoch_ending_time)))\n",
    "            print(\"\\tTotal time elapsed: {}\".format(self.train_timer(self.beginning_time, time.time())))\n",
    "        \n",
    "        # capture globaliter\n",
    "        torch.save(self.globaliter, os.path.join(self.object_dir, \"globaliter.pt\"))\n",
    "\n",
    "    def validation(self, epoch):\n",
    "        epoch_preds = []\n",
    "        epoch_targets = []\n",
    "\n",
    "        epoch_f1 = []\n",
    "        epoch_precision = []\n",
    "        epoch_recall = []\n",
    "        epoch_accuracy = []\n",
    "        epoch_loss = []\n",
    "        \n",
    "        # sample batch number for data capture\n",
    "        num_batches = np.floor(len(self.validation_data_loader.dataset.image_paths) / self.validation_data_loader.batch_size)\n",
    "        sample_batch_idx = np.random.randint(0, num_batches)\n",
    "\n",
    "        # turn off gradients\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "\n",
    "            for batch_idx, (data, target) in enumerate(self.validation_data_loader):\n",
    "                # reshape data as needed and send data to GPU if available\n",
    "                data = data.to(self.device)\n",
    "                target = target.to(self.device)\n",
    "\n",
    "                # generate predictions\n",
    "                output = self.model(data)\n",
    "\n",
    "                validation_loss = self.criterion(output, target)\n",
    "                epoch_loss.append(validation_loss.item())\n",
    "\n",
    "                #Metrics\n",
    "                _, pred = torch.max(output, dim=1)\n",
    "                epoch_preds = epoch_preds + pred.detach().cpu().numpy().tolist()\n",
    "                epoch_targets = epoch_targets + target.detach().cpu().numpy().tolist()\n",
    "\n",
    "                metric_f1 = f1_score(epoch_targets, epoch_preds)\n",
    "                epoch_f1.append(metric_f1)\n",
    "\n",
    "                metric_precision = precision_score(epoch_targets, epoch_preds)\n",
    "                epoch_precision.append(metric_precision)\n",
    "\n",
    "                metric_recall = recall_score(epoch_targets, epoch_preds)\n",
    "                epoch_recall.append(metric_recall)\n",
    "\n",
    "                metric_accuracy = accuracy_score(epoch_targets, epoch_preds)\n",
    "                epoch_accuracy.append(metric_accuracy)\n",
    "            \n",
    "            # #\n",
    "            # if batch_idx == sample_batch_idx:\n",
    "            #     image_grid = torchvision.utils.make_grid(data.cpu())\n",
    "            #     self.validation_summary_writer.add_image('validation/Sample batch', image_grid, global_step=self.globaliter)\n",
    "                \n",
    "            # \n",
    "            try:\n",
    "                self.running_avg_validation_f1.append((sum(epoch_f1) / len(epoch_f1)))\n",
    "                self.running_avg_validation_precision.append((sum(epoch_precision) / len(epoch_precision)))\n",
    "                self.running_avg_validation_recall.append((sum(epoch_recall) / len(epoch_recall)))\n",
    "                self.running_avg_validation_accuracy.append((sum(epoch_accuracy) / len(epoch_accuracy)))\n",
    "                self.running_avg_validation_loss.append((sum(epoch_loss) / len(epoch_loss)))\n",
    "\n",
    "                torch.save(self.running_avg_validation_f1, os.path.join(self.object_dir, \"running_avg_validation_f1.pt\"))\n",
    "                torch.save(self.running_avg_validation_precision, os.path.join(self.object_dir, \"running_avg_validation_precision.pt\"))\n",
    "                torch.save(self.running_avg_validation_recall, os.path.join(self.object_dir, \"running_avg_validation_recall.pt\"))\n",
    "                torch.save(self.running_avg_validation_accuracy, os.path.join(self.object_dir, \"running_avg_validation_accuracy.pt\"))\n",
    "                torch.save(self.running_avg_validation_loss, os.path.join(self.object_dir, \"running_avg_validation_loss.pt\"))\n",
    "\n",
    "                # tensorboard\n",
    "                if self.tensorboard_files:\n",
    "                    # validation panel - one scalar per metric per plot\n",
    "                    self.validation_summary_writer.add_scalar('validation/F1', self.running_avg_validation_f1[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Precision', self.running_avg_validation_precision[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Recall', self.running_avg_validation_recall[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Accuracy', self.running_avg_validation_accuracy[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('validation/Loss', self.running_avg_validation_loss[-1], global_step=self.globaliter)\n",
    "\n",
    "                    # metric-specific plots\n",
    "                    self.validation_summary_writer.add_scalar('F1', self.running_avg_validation_f1[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Precision', self.running_avg_validation_precision[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Recall', self.running_avg_validation_recall[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Accuracy', self.running_avg_validation_accuracy[-1], global_step=self.globaliter)\n",
    "                    self.validation_summary_writer.add_scalar('Loss', self.running_avg_validation_loss[-1], global_step=self.globaliter)\n",
    "                    \n",
    "                    self.validation_summary_writer.flush()\n",
    "\n",
    "            # create statistics object and continue\n",
    "            except AttributeError:\n",
    "                self.running_avg_validation_f1 = [(sum(epoch_f1) / len(epoch_f1))]\n",
    "                self.running_avg_validation_precision = [(sum(epoch_precision) / len(epoch_precision))]\n",
    "                self.running_avg_validation_recall = [(sum(epoch_recall) / len(epoch_recall))]\n",
    "                self.running_avg_validation_accuracy = [(sum(epoch_accuracy) / len(epoch_accuracy))]\n",
    "                self.running_avg_validation_loss = [(sum(epoch_loss) / len(epoch_loss))]\n",
    "                \n",
    "            # print progress report\n",
    "            if self.verbose:\n",
    "                print(\"\\nValidation epoch: {} \\n\\tLoss: {:.6f} | F1: {:.6f} | Precision: {:.6f} | Recall: {:.6f} | Accuracy: {:.6f}\\n\".format(\n",
    "                        epoch, self.running_avg_validation_loss[-1], self.running_avg_validation_f1[-1],\n",
    "                        self.running_avg_validation_precision[-1], self.running_avg_validation_recall[-1], self.running_avg_validation_accuracy[-1]))\n",
    "        \n",
    "            # early stopping\n",
    "            if self.running_avg_validation_loss[-1] < self.min_val_loss:\n",
    "                # Save the model checkpoint\n",
    "                torch.save(self.model.state_dict(), os.path.join(self.model_dir, \"{}.pt\".format(self.model_name)))\n",
    "                self.epochs_no_improve = 0\n",
    "                self.min_val_loss = self.running_avg_validation_loss[-1]\n",
    "                \n",
    "                if self.verbose:\n",
    "                    print(\">>> Improved - saving model\\n\\n\\n\")\n",
    "\n",
    "            else:\n",
    "                self.epochs_no_improve += 1\n",
    "                if self.verbose:\n",
    "                    print(\">>> No improvement - {} consecutive epochs\\n\\n\\n\".format(self.epochs_no_improve))\n",
    "                if self.epochs_no_improve == self.n_epochs_stop:\n",
    "                    if self.verbose:\n",
    "                        print(\"\\n!!! Early stopping - {} epochs without improvement\\n\".format(self.n_epochs_stop))\n",
    "                self.running_avg_validation_loss = []\n",
    "                    \n",
    "    def train_timer(self, start, end):\n",
    "        hours, rem = divmod(end-start, 3600)\n",
    "        minutes, seconds = divmod(rem, 60)\n",
    "        return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours),int(minutes),seconds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "https://www.kaggle.com/xinruizhuang/skin-lesion-classification-acc-90-pytorch\n",
    "\n",
    "https://pytorch.org/tutorials/intermediate/tensorboard_tutorial.html\n",
    "\n",
    "https://github.com/yunjey/pytorch-tutorial/tree/master/tutorials/04-utils/tensorboard\n",
    "\n",
    "https://pytorch.org/docs/stable/tensorboard.html\n",
    "\n",
    "https://towardsdatascience.com/transfer-learning-with-convolutional-neural-networks-in-pytorch-dd09190245ce\n",
    "\n",
    "https://towardsdatascience.com/https-medium-com-dinber19-take-a-deeper-look-at-your-pytorch-model-with-the-new-tensorboard-built-in-513969cf6a72\n",
    "\n",
    "https://github.com/andyhahaha/Uncertainty-Mnist-with-Pytorch\n",
    "\n",
    "https://discuss.pytorch.org/t/using-nn-dropout2d-at-eval-time-for-modelling-uncertainty/45274\n",
    "\n",
    "https://xuwd11.github.io/Dropout_Tutorial_in_PyTorch/\n",
    "\n",
    "https://towardsdatascience.com/making-your-neural-network-say-i-dont-know-bayesian-nns-using-pyro-and-pytorch-b1c24e6ab8cd\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
